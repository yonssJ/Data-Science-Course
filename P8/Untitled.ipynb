{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff2c7a-9f2b-44c5-a8ea-d6e1e0d27f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /home/yonss/anaconda3/lib/python3.9/site-packages (2.0.1)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
      "\u001b[K     |█▎                              | 11.2 MB 19 kB/s eta 3:51:006"
     ]
    }
   ],
   "source": [
    "!python -m pip install findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30797b8-ae17-4992-978d-5c3393c2db92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SPARK_HOME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_286838/3703800614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ensure SPARK_HOME is correctly set (in .bashrc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SPARK_HOME'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SPARK_HOME'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# ensure SPARK_HOME is correctly set (in .bashrc)\n",
    "os.environ['SPARK_HOME']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44ffde0-f7dc-4185-abb9-67c05c0c3579",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_286838/616703944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fonctions pyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloatType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Fonctions pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, DataType, FloatType\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "\n",
    "\n",
    "# Fonction pour ouvrir l'image à partir de son chemin d'accès\n",
    "from PIL import Image\n",
    "\n",
    "# Librairies classiques\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Librairie pour se connecter au service S3 d'AWS\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb35ff5-b573-47c8-99f9-7d409f8e39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datas(folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Retourne un dataFrame pyspark avec comme colonne la liste des chemins d'accès\n",
    "    de toutes les images se trouvant le dossier folder\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation du temps de calcul\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lst_path =  []\n",
    "    \n",
    "    # Suivant l'argument sys.argv[1], connexion en local ou sur AWS\n",
    "    if sys.argv[1] == 'True':\n",
    "\n",
    "        sub_folders = os.listdir(folder)\n",
    "        print(sub_folders)\n",
    "\n",
    "        for f in sub_folders:\n",
    "\n",
    "            lst_categ = os.listdir(folder + f)\n",
    "\n",
    "            for file in lst_categ:\n",
    "                lst_path.append(folder + f + \"/\" + file)\n",
    "    else :\n",
    "        # Connexion à l'espace de stockage S3 d'AWS\n",
    "        session = boto3.session.Session(aws_access_key_id=\"\",\n",
    "                                        aws_secret_access_key=\"\")\n",
    "        s3_client = session.client(service_name='s3', region_name=\"us-east-1\")\n",
    "\n",
    "        prefix = 'data'\n",
    "        sub_folders = s3_client.list_objects_v2(Bucket=\"mlgbucket\", Prefix=prefix)\n",
    "\n",
    "        if \"Contents\" not in sub_folders:\n",
    "            print(\"Erreur lors du chargement des images\")\n",
    "            print(\"Le dossier source n'a pas été trouvé\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        for key in sub_folders[\"Contents\"]:\n",
    "\n",
    "            file = key[\"Key\"]\n",
    "            file = file.replace(prefix + \"/\", \"\")\n",
    "            lst_path.append(folder + file)\n",
    "\n",
    "    print(\"Nombre d'images chargées :\", len(lst_path))\n",
    "    # Création d'un RDD à partit de la liste des chemins d'accès aux images\n",
    "    rdd = sc.parallelize(lst_path)\n",
    "    row_rdd = rdd.map(lambda x: Row(x))\n",
    "    # Création d'un dataFrame pyspark à partir d'un RDD\n",
    "    df = spark.createDataFrame(row_rdd, [\"path_img\"])\n",
    "\n",
    "    # Affichage du temps de calcul\n",
    "    print(\"Temps d'execution {:.2f} secondes\".format(time.time() - start_time))\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
